from Bio import SeqIO
import math
from multiprocessing import Pool
import numpy as np
import os
import pandas as pd
import pickle as pkl
import random
from sklearn.metrics import precision_score, recall_score
import subprocess
import  torch
from    torch import nn
from    torch import optim
import  torch.utils.data as Data
from trans_model import Transformer


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def build_pc_db(input_prot_path, ref_dir, threads=8):
    """Build protein cluster database.
    """
    if not os.path.exists(ref_dir):
        os.makedirs(ref_dir)

    # run all-against-all alignment using DIAMOND
    subprocess.run(f"diamond makedb --in {input_prot_path} -d {ref_dir}/prot_db -p {threads}", shell=True)
    subprocess.run(f"diamond blastp -d {ref_dir}/prot_db -q {input_prot_path} -o {ref_dir}/prot.blastp -p {threads} -f 6 --sensitive", shell=True)
    
    # run mcl to generate the protein cluster
    with open(f"{ref_dir}/prot.abc", 'w') as abc_rst:
        with open(f"{ref_dir}/prot.blastp") as blastp_rst:
            for l in blastp_rst:
                l = l.strip().split()
                if l[0] != l[1]:
                    abc_rst.write(f"{l[0]} {l[1]} {l[10]}\n")
    subprocess.run(f"mcxload -abc {ref_dir}/prot.abc --stream-mirror --stream-neg-log10 -stream-tf 'ceil(200)' \
                   -o {ref_dir}/prot.mci -write-tab {ref_dir}/prot.tab", shell=True)
    subprocess.run(f"mcl {ref_dir}/prot.mci -I 2.0 -use-tab {ref_dir}/prot.tab -o {ref_dir}/prot.clusters -te {threads}", shell=True)
    
    # generate the PC profile files
    pc_index = 0
    pc_dict = {}
    with open(f"{ref_dir}/prot.clusters") as cls_p:
        for l in cls_p:
            pc_list = l.strip().split('\t')
            if len(pc_list) > 1:
                pc_dict[pc_index] = pc_list
                pc_index += 1

    with open(f"{ref_dir}/prot.p2a", 'w') as p_out:
        for pc_i, s_list in pc_dict.items():
            for s_id in s_list:
                p_out.write(f"PC_{pc_i:06d},{s_id}\n")


def split_fasta(fasta_file, num_split=10):
    """Split original fasta file into several fasta files.
    """
    all_seq_id = [s.id for s in SeqIO.parse(fasta_file, 'fasta')]
    random.shuffle(all_seq_id)

    num_per_split = math.ceil(len(all_seq_id)/ num_split)

    num_files = 0
    seq_index = SeqIO.index(fasta_file, 'fasta')
    for i in range(0, len(all_seq_id), num_per_split):
        if len(all_seq_id[i: i + num_per_split]) > 0:
            out_seq = [seq_index[s] for s in all_seq_id[i: i + num_per_split]]
            SeqIO.write(out_seq, f"{fasta_file}.{num_files}", 'fasta')
            num_files += 1

    return num_files


def prodigal(dna_path):
    """Run prodigal.
    """
    subprocess.run(f"prodigal -i {dna_path} -o {dna_path}.gff -a {dna_path}.aa_ -f gff -p meta -q", shell=True)


def run_multi_prodigal(contig_path, threads=32):
    """Run prodigal in multiple threads.
    """
    num_files = split_fasta(fasta_file=contig_path, num_split=threads)
    threads = num_files
    pool = Pool(processes=threads)
    for temp_id in range(threads):
        pool.apply_async(prodigal, [f"{contig_path}.{temp_id}"])
    pool.close()
    pool.join()
    
    # need to merge and delete the temporary files
    # ref_dir = os.path.dirname(contig_path)
    for i in range(threads):
        os.remove(f"{contig_path}.{i}")

    # remove the * in the last
    for i in range(threads):
        subprocess.run(f"sed 's/*$//g' {contig_path}.{i}.aa_ > {contig_path}.{i}.aa", shell=True)
        os.remove(f"{contig_path}.{i}.aa_")
    
    # merge the files
    with open(f'{contig_path}.aa', 'w') as outfile:
        for i in range(threads):
            fname = f"{contig_path}.{i}.aa"
            with open(fname) as infile:
                for line in infile:
                    outfile.write(line)
            os.remove(fname)

    with open(f'{contig_path}.gff', 'w') as outfile:
        for i in range(threads):
            fname = f"{contig_path}.{i}.gff"
            with open(fname) as infile:
                for line in infile:
                    outfile.write(line)
            os.remove(fname)


def count_aa(aa_fasta):
    """Count the number of aa generated by prodigal.
    """
    aa_num_dict = {}
    for s in SeqIO.parse(aa_fasta, 'fasta'):
        query_name = s.id.rsplit('_', 1)[0]
        if query_name in aa_num_dict:
            aa_num_dict[query_name] += 1
        else:
            aa_num_dict[query_name] = 1

    with open(f"{aa_fasta}.aa_count", 'w') as aa_c:
        for k, v in aa_num_dict.items():
            aa_c.write(f"{k}\t{v}\n")


def ntseq2vector(input_nt_path, ref_dir, out_dir, vec_len=400, threads=8):
    """Convert the input sequences into encode vector.
    """
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    # predict proteins from the input nucleotide sequences
    run_multi_prodigal(contig_path=input_nt_path, 
                       threads=threads)
    contig_aa_path = f"{input_nt_path}.aa"
    count_aa(aa_fasta=contig_aa_path)
    aa_count_path = f"{input_nt_path}.aa.aa_count"
    
    # built the dictionay of query: the number of protein
    num_aa_dict = {}
    with open(aa_count_path) as acp:
        for l in acp:
            l = l.split()
            num_aa_dict[l[0]] = int(l[1])
    
    # align the predicted proteins to the PC database
    protein_path = f"{input_nt_path}.aa"
    blastp_path = f"{input_nt_path}.aa.blastp"
    subprocess.run(f"diamond blastp -d {ref_dir}/prot_db -q {protein_path} -o {blastp_path} -p {threads} -f 6 --sensitive", 
                   shell=True)
    
    # load the protein cluster profiles
    p2a_path = f"{ref_dir}/prot.p2a"
    pc2db_aa_df = pd.read_csv(p2a_path, sep=',', header=None)
    pc2wordsid = {pc: idx for idx, pc in enumerate(sorted(set(pc2db_aa_df[0].values)))}
    protein2pc = {protein: pc for protein, pc in zip(pc2db_aa_df[1].values, pc2db_aa_df[0].values)}

    # parse the blastp results and generate the encoding vectors
    query_dict = {}
    with open(blastp_path) as bp:
        for l in bp:
            record = l.split()
            query = record[0]
            ref = record[1]
            ident = float(record[2])
            evalue = float(record[10])
            if query in query_dict:
                if ref in protein2pc:
                    if evalue < query_dict[query][1]:
                        query_dict[query] = [ref, evalue, ident]
            else:
                query_dict[query] = [ref, evalue, ident]
    
    contig2pcs = {}

    for query, info in query_dict.items():
        contig = query.rsplit('_', 1)[0]
        idx    = int(query.rsplit('_', 1)[1]) - 1

        ref = info[0]
        evalue = info[1]
        ident = info[2]

        try:
            pc = pc2wordsid[protein2pc[ref]]
        except KeyError:
            continue
        
        try:
            contig2pcs[contig].append((idx, pc, evalue))
        except:
            contig2pcs[contig] = [(idx, pc, evalue)]

    # sorted the proteins by position
    contig_id_list_f = open(out_dir + '/' + 'sentence_id.list', 'w')
    for contig in contig2pcs:
        contig2pcs[contig] = sorted(contig2pcs[contig], key=lambda tup: tup[0])
        contig_id_list_f.write(f"{contig}\n")
    contig_id_list_f.close()

    # convert the input nucleotide sequences into vector
    contig2id = {contig:idx for idx, contig in enumerate(contig2pcs.keys())}
    id2contig = {idx:contig for idx, contig in enumerate(contig2pcs.keys())}
    sentence = np.zeros((len(contig2id.keys()), vec_len))
    for row in range(sentence.shape[0]):
        contig = id2contig[row]
        pcs = contig2pcs[contig]

        for i in range(num_aa_dict[contig]):
            if i < vec_len:
                sentence[row][i] = 1
        
        for (idx, pc, pc_w) in pcs:
            if idx < vec_len:
                sentence[row][idx] = pc + 2

    # Store the parameters
    pkl.dump(sentence, open(out_dir + '/' + 'sentence.feat', 'wb'))
    pkl.dump(id2contig, open(out_dir + '/' + 'sentence_id2contig.dict', 'wb'))
    pkl.dump(pc2wordsid, open(out_dir + '/' + 'pc2wordsid.dict', 'wb'))


def reset_model(vocab_size, pad_idx, max_len):
    """Set the Transformer model.
    """
    model = Transformer(
                src_vocab_size=vocab_size, 
                src_pad_idx=pad_idx, 
                num_layers=1,
                device=device, 
                max_length=max_len, 
                dropout=0.1
    ).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss_func = nn.BCEWithLogitsLoss()
    return model, optimizer, loss_func


def return_batch(train_sentence, label, flag):
    """Return the batch of data and labels.
    """
    X_train = torch.from_numpy(train_sentence)
    y_train = torch.from_numpy(label).float()
    train_dataset = Data.TensorDataset(X_train, y_train)
    training_loader = Data.DataLoader(
        dataset=train_dataset,    
        batch_size=256,
        shuffle=flag,               
        num_workers=0,              
    )
    return training_loader


def train(pos_dir, neg_dir, val_pos_dir, val_neg_dir, model_path, num_epoch=30):
    """Train the PC-based Transfer model.
    """
    # load positive and negtive training data
    pcs2idx = pkl.load(open(f'{pos_dir}/pc2wordsid.dict', 'rb'))
    train_pos = pkl.load(open(f'{pos_dir}/sentence.feat', 'rb'))
    train_neg = pkl.load(open(f'{neg_dir}/sentence.feat', 'rb'))
    train = np.vstack((train_pos, train_neg))
    train_label = np.concatenate((np.ones(train_pos.shape[0]), np.zeros(train_neg.shape[0])))
    training_loader = return_batch(train, train_label, flag = True)

    # load positive and negtive validation data
    val_pos = pkl.load(open(f'{val_pos_dir}/sentence.feat', 'rb'))
    val_neg = pkl.load(open(f'{val_neg_dir}/sentence.feat', 'rb'))
    validation = np.vstack((val_pos, val_neg))
    val_label = np.concatenate((np.ones(val_pos.shape[0]), np.zeros(val_neg.shape[0])))
    val_loader = return_batch(validation, val_label, flag = True)

    # load the label and dataset
    num_pcs = len(set(pcs2idx.keys()))
    src_vocab_size = num_pcs + 2
    src_pad_idx = 0

    # train
    model, optimizer, loss_func = reset_model(vocab_size=src_vocab_size, 
                                              pad_idx=src_pad_idx,
                                              max_len=400)

    for epoch in range(num_epoch):
        _ = model.train()
        for step, (batch_x, batch_y) in enumerate(training_loader): 
            
            sentense = batch_x.to(device).to(torch.int64)
            batch_y = batch_y.to(device)

            prediction = model(sentense)
            loss = loss_func(prediction.squeeze(1), batch_y)
            optimizer.zero_grad()
            loss.backward() 
            optimizer.step()

        _ = model.eval()
        with torch.no_grad():
            all_pred = []
            test_label = []
            all_logit = []
            for step, (batch_x, batch_y) in enumerate(val_loader):
                # sentense = batch_x.to(torch.int64)
                sentense = batch_x.to(device).to(torch.int64)
                batch_y = batch_y.to(device)

                logit = model(sentense)
                logit  = torch.sigmoid(logit.squeeze(1))
                pred  = [1 if item > 0.5 else 0 for item in logit]
                all_pred += pred
                all_logit += [i for i in logit]
                test_label += batch_y.tolist()
            precision = precision_score(test_label, all_pred)
            recall = recall_score(test_label, all_pred)

        print(f'epoch no. {epoch} || precision: {precision} || recall: {recall}')

    torch.save(model.state_dict(), model_path)


# test the performance of the model
def test(data_loader, model, logit_thres=0.5):
    model.eval()
    all_pred = []
    all_logit = []

    for (batch_x, _) in data_loader:
        sentense = batch_x.to(device).to(torch.int64)

        with torch.no_grad():
            logit = model(sentense)
            logit  = torch.sigmoid(logit.squeeze(1)).cpu().detach().numpy()
            pred  = [1 if item > logit_thres else 0 for item in logit]
            all_pred += pred
            all_logit += [i for i in logit]

    return all_pred, all_logit


def predict(test_dir, model_path, pred_ouput, logit_thres=0.5):
    """Test the model performance.
    """
    # load the token profiles
    pcs2idx = pkl.load(open(f'{test_dir}/pc2wordsid.dict', 'rb'))
    num_pcs = len(set(pcs2idx.keys()))
    src_vocab_size = num_pcs+2
    src_pad_idx = 0

    # load the test data
    test_feat = pkl.load(open(f'{test_dir}/sentence.feat', 'rb'))
    test_seq_list = []
    with open(f"{test_dir}/sentence_id.list") as sent_list:
        for l in sent_list:
            test_seq_list.append(l.strip())

    # predict the results
    with open(pred_ouput, 'w') as po:
        temp_label = np.ones((len(test_feat)))
        test_loader = return_batch(np.array(test_feat), temp_label, flag=False)

        model, _, _ = reset_model(vocab_size=src_vocab_size, 
                                pad_idx=src_pad_idx,
                                max_len=400)

        model.load_state_dict(torch.load(model_path, map_location=device))
        y_pred, y_logit = test(test_loader, model, logit_thres)
        
        for s, pred, logit in zip(test_seq_list, y_pred, y_logit):
            po.write(f"{s}\t{pred}\t{logit}\n")


if __name__ == "__main__":
    # Initialization
    num_threads = 8 # the number of threads
    len_vec = 400   # the length of encoded vector
    num_epoch = 5 # the number of training epochs

    ref_dir = f"path/to/ref_dir"    # the directory for storing references
    ref_proteins = f"path/to/ref_protein.faa"   # the path of reference proteins

    train_pos_path = f"path/to/pos.fna" # the path of positive training set
    train_pos_data_dir = f"path/to/pos" # the directory for storing positive training data
    train_neg_path = f"path/to/neg.fna" # the path of negative training set
    train_neg_data_dir = f"path/to/neg" # the directory for storing negative training data

    val_pos_path = f"path/to/val_pos.fna" # the path of positive validation set
    val_pos_data_dir = f"path/to/val_pos" # the directory for storing positive validation data
    val_neg_path = f"path/to/val_neg.fna" # the path of negative validation set
    val_neg_data_dir = f"path/to/val_neg" # the directory for storing negative validation data

    model_path = f"path/to/model.pt"    # the path of trained model

    test_path = f"path/to/test.fna" # the path of testing set
    test_data_dir = f"path/to/test" # the directory for storing testing data
    test_pred_rst = f"path/to/test.csv" # the path of prediction results

    # Generate protein clusters
    build_pc_db(input_prot_path=ref_proteins, 
                ref_dir=ref_dir, 
                threads=num_threads)

    # Build training and validation data
    ntseq2vector(input_nt_path=train_pos_path, ref_dir=ref_dir, 
                 out_dir=train_pos_data_dir, vec_len=len_vec, threads=num_threads)
    ntseq2vector(input_nt_path=train_neg_path, ref_dir=ref_dir, 
                 out_dir=train_neg_data_dir, vec_len=len_vec, threads=num_threads)
    ntseq2vector(input_nt_path=val_pos_path, ref_dir=ref_dir, 
                 out_dir=val_pos_data_dir, vec_len=len_vec, threads=num_threads)
    ntseq2vector(input_nt_path=val_neg_path, ref_dir=ref_dir, 
                 out_dir=val_neg_data_dir, vec_len=len_vec, threads=num_threads)

    # Train the Transformer model
    train(pos_dir=train_pos_data_dir, neg_dir=train_neg_data_dir, 
          val_pos_dir=val_pos_data_dir, val_neg_dir=val_neg_data_dir, 
          model_path=model_path, num_epoch=3)

    # Predict the labels of testing data
    ntseq2vector(input_nt_path=test_path, ref_dir=ref_dir, 
                 out_dir=test_data_dir, vec_len=len_vec, threads=num_threads)
    predict(test_dir=test_data_dir, model_path=model_path, 
            pred_ouput=test_pred_rst, logit_thres=0.5)